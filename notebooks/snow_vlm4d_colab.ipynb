{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNOW VLM4D Evaluation on Google Colab\n",
    "\n",
    "This notebook runs the full SNOW pipeline on VLM4D benchmark:\n",
    "1. Video frames → MapAnything → 3D point clouds\n",
    "2. Point clouds → HDBSCAN clustering → Object segmentation\n",
    "3. Objects → Cross-frame tracking → Temporal tracks\n",
    "4. Tracks → 4D Scene Graph → Text serialization\n",
    "5. Text 4DSG → **Gemma3-4B-IT** → Answer\n",
    "\n",
    "**Requirements:** GPU runtime (T4 recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision\n",
    "!pip install -q hdbscan scipy numpy opencv-python pillow\n",
    "!pip install -q google-genai\n",
    "!pip install -q huggingface_hub transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Google AI API Key\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Option 1: Use Colab secrets (recommended)\n",
    "try:\n",
    "    os.environ['GOOGLE_AI_API_KEY'] = userdata.get('GOOGLE_AI_API_KEY')\n",
    "except:\n",
    "    # Option 2: Set manually\n",
    "    os.environ['GOOGLE_AI_API_KEY'] = 'YOUR_API_KEY_HERE'  # Replace with your key\n",
    "\n",
    "print(\"API Key set:\", \"Yes\" if os.environ.get('GOOGLE_AI_API_KEY') else \"No\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define SNOW Core Components\n",
    "\n",
    "Since we can't clone a private repo, we define the essential SNOW components inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import tempfile\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from enum import Enum\n",
    "import json\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Token Definitions ====\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PatchToken:\n",
    "    row: int\n",
    "    col: int\n",
    "    iou: float\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class CentroidToken:\n",
    "    x: float\n",
    "    y: float\n",
    "    z: float\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ShapeToken:\n",
    "    x_mu: float\n",
    "    x_sigma: float\n",
    "    x_min: float\n",
    "    x_max: float\n",
    "    y_mu: float\n",
    "    y_sigma: float\n",
    "    y_min: float\n",
    "    y_max: float\n",
    "    z_mu: float\n",
    "    z_sigma: float\n",
    "    z_min: float\n",
    "    z_max: float\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TemporalToken:\n",
    "    t_start: int\n",
    "    t_end: int\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class STEPToken:\n",
    "    patch_tokens: List[PatchToken]\n",
    "    centroid: CentroidToken\n",
    "    shape: ShapeToken\n",
    "    temporal: TemporalToken\n",
    "\n",
    "def mask_to_patch_tokens(mask, grid_size=16, iou_threshold=0.5):\n",
    "    H, W = mask.shape\n",
    "    patch_h, patch_w = H // grid_size, W // grid_size\n",
    "    tokens = []\n",
    "    for row in range(grid_size):\n",
    "        for col in range(grid_size):\n",
    "            y_start, y_end = row * patch_h, (row + 1) * patch_h\n",
    "            x_start, x_end = col * patch_w, (col + 1) * patch_w\n",
    "            patch = mask[y_start:y_end, x_start:x_end]\n",
    "            iou = patch.sum() / patch.size if patch.size > 0 else 0\n",
    "            if iou > iou_threshold:\n",
    "                tokens.append(PatchToken(row=row, col=col, iou=float(iou)))\n",
    "    return tokens\n",
    "\n",
    "def build_centroid_token(points_xyz):\n",
    "    if len(points_xyz) == 0:\n",
    "        return CentroidToken(0, 0, 0)\n",
    "    mean = points_xyz.mean(axis=0)\n",
    "    return CentroidToken(float(mean[0]), float(mean[1]), float(mean[2]))\n",
    "\n",
    "def build_shape_token(points_xyz):\n",
    "    if len(points_xyz) == 0:\n",
    "        return ShapeToken(0,0,0,0, 0,0,0,0, 0,0,0,0)\n",
    "    return ShapeToken(\n",
    "        x_mu=float(points_xyz[:,0].mean()), x_sigma=float(points_xyz[:,0].std()),\n",
    "        x_min=float(points_xyz[:,0].min()), x_max=float(points_xyz[:,0].max()),\n",
    "        y_mu=float(points_xyz[:,1].mean()), y_sigma=float(points_xyz[:,1].std()),\n",
    "        y_min=float(points_xyz[:,1].min()), y_max=float(points_xyz[:,1].max()),\n",
    "        z_mu=float(points_xyz[:,2].mean()), z_sigma=float(points_xyz[:,2].std()),\n",
    "        z_min=float(points_xyz[:,2].min()), z_max=float(points_xyz[:,2].max()),\n",
    "    )\n",
    "\n",
    "def build_step_token(mask, points_xyz, t_start, t_end, grid_size=16, iou_threshold=0.5):\n",
    "    return STEPToken(\n",
    "        patch_tokens=mask_to_patch_tokens(mask, grid_size, iou_threshold),\n",
    "        centroid=build_centroid_token(points_xyz),\n",
    "        shape=build_shape_token(points_xyz),\n",
    "        temporal=TemporalToken(t_start, t_end),\n",
    "    )\n",
    "\n",
    "print(\"Token components defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== HDBSCAN Clustering ====\n",
    "import hdbscan\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class HDBSCANConfig:\n",
    "    min_cluster_size: int = 30\n",
    "    min_samples: int = 5\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ClusterResult:\n",
    "    labels: np.ndarray\n",
    "    clusters: List[np.ndarray]\n",
    "\n",
    "def cluster_points(points_xyz, config):\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=config.min_cluster_size,\n",
    "        min_samples=config.min_samples,\n",
    "    )\n",
    "    labels = clusterer.fit_predict(points_xyz)\n",
    "    clusters = []\n",
    "    for label in sorted(set(labels)):\n",
    "        if label == -1:\n",
    "            continue\n",
    "        idx = np.where(labels == label)[0]\n",
    "        clusters.append(idx)\n",
    "    return ClusterResult(labels=labels, clusters=clusters)\n",
    "\n",
    "print(\"HDBSCAN clustering defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== H-hop Filter ====\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class HHopConfig:\n",
    "    max_extent: float = 50.0\n",
    "    max_sigma: float = 10.0\n",
    "    max_aspect_ratio: float = 20.0\n",
    "\n",
    "def filter_implausible(steps, config):\n",
    "    \"\"\"Filter out geometrically implausible STEP tokens.\"\"\"\n",
    "    valid = {}\n",
    "    for k, step in steps.items():\n",
    "        s = step.shape\n",
    "        extents = [s.x_max - s.x_min, s.y_max - s.y_min, s.z_max - s.z_min]\n",
    "        max_ext = max(extents)\n",
    "        min_ext = min(e for e in extents if e > 0.01) if any(e > 0.01 for e in extents) else 1\n",
    "        aspect = max_ext / min_ext if min_ext > 0 else 1\n",
    "        \n",
    "        if max_ext > config.max_extent:\n",
    "            continue\n",
    "        if aspect > config.max_aspect_ratio:\n",
    "            continue\n",
    "        if max(s.x_sigma, s.y_sigma, s.z_sigma) > config.max_sigma:\n",
    "            continue\n",
    "        valid[k] = step\n",
    "    return valid\n",
    "\n",
    "print(\"H-hop filter defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Object Tracker ====\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrackerConfig:\n",
    "    geometric_weight: float = 0.5\n",
    "    semantic_weight: float = 0.5\n",
    "    max_centroid_distance: float = 5.0\n",
    "    max_association_cost: float = 2.0\n",
    "    max_age: int = 5\n",
    "\n",
    "@dataclass\n",
    "class Track:\n",
    "    track_id: int\n",
    "    steps: List[STEPToken] = field(default_factory=list)\n",
    "    age: int = 0\n",
    "    \n",
    "    def update(self, step):\n",
    "        self.steps.append(step)\n",
    "        self.age = 0\n",
    "\n",
    "class ObjectTracker:\n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or TrackerConfig()\n",
    "        self.tracks = {}\n",
    "        self.next_id = 0\n",
    "    \n",
    "    def update(self, detections, frame_idx):\n",
    "        if not self.tracks:\n",
    "            for det_id, step in detections.items():\n",
    "                track = Track(track_id=self.next_id)\n",
    "                track.update(step)\n",
    "                self.tracks[self.next_id] = track\n",
    "                self.next_id += 1\n",
    "            return\n",
    "        \n",
    "        # Simple nearest-neighbor matching by centroid\n",
    "        det_list = list(detections.values())\n",
    "        track_list = list(self.tracks.values())\n",
    "        \n",
    "        if not det_list:\n",
    "            for t in track_list:\n",
    "                t.age += 1\n",
    "            return\n",
    "        \n",
    "        cost = np.zeros((len(det_list), len(track_list)))\n",
    "        for i, det in enumerate(det_list):\n",
    "            for j, track in enumerate(track_list):\n",
    "                if track.steps:\n",
    "                    prev = track.steps[-1]\n",
    "                    d = np.sqrt(\n",
    "                        (det.centroid.x - prev.centroid.x)**2 +\n",
    "                        (det.centroid.y - prev.centroid.y)**2 +\n",
    "                        (det.centroid.z - prev.centroid.z)**2\n",
    "                    )\n",
    "                    cost[i, j] = d\n",
    "                else:\n",
    "                    cost[i, j] = 1e6\n",
    "        \n",
    "        row_ind, col_ind = linear_sum_assignment(cost)\n",
    "        matched_dets = set()\n",
    "        matched_tracks = set()\n",
    "        \n",
    "        for r, c in zip(row_ind, col_ind):\n",
    "            if cost[r, c] < self.config.max_centroid_distance:\n",
    "                track_list[c].update(det_list[r])\n",
    "                matched_dets.add(r)\n",
    "                matched_tracks.add(c)\n",
    "        \n",
    "        # New tracks for unmatched detections\n",
    "        for i, det in enumerate(det_list):\n",
    "            if i not in matched_dets:\n",
    "                track = Track(track_id=self.next_id)\n",
    "                track.update(det)\n",
    "                self.tracks[self.next_id] = track\n",
    "                self.next_id += 1\n",
    "        \n",
    "        # Age unmatched tracks\n",
    "        for j, track in enumerate(track_list):\n",
    "            if j not in matched_tracks:\n",
    "                track.age += 1\n",
    "    \n",
    "    def get_tracks(self):\n",
    "        return {tid: t for tid, t in self.tracks.items() if t.age <= self.config.max_age}\n",
    "\n",
    "print(\"Object tracker defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Scene Graph & 4DSG ====\n",
    "\n",
    "class HorizontalRelation(Enum):\n",
    "    LEFT = \"left\"\n",
    "    RIGHT = \"right\"\n",
    "    FRONT = \"front\"\n",
    "    BACK = \"back\"\n",
    "\n",
    "class VerticalRelation(Enum):\n",
    "    ABOVE = \"above\"\n",
    "    BELOW = \"below\"\n",
    "    LEVEL = \"level\"\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SceneNode:\n",
    "    node_id: int\n",
    "    step: STEPToken\n",
    "    position: np.ndarray\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SceneEdge:\n",
    "    src: int\n",
    "    dst: int\n",
    "    distance: float\n",
    "    relation: str\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SceneGraph:\n",
    "    nodes: Dict[int, SceneNode]\n",
    "    edges: List[SceneEdge]\n",
    "    frame_idx: int\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TemporalTrack:\n",
    "    track_id: int\n",
    "    steps: List[STEPToken]\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TemporalWindow:\n",
    "    tracks: Dict[int, TemporalTrack]\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class FourDSceneGraph:\n",
    "    spatial_graphs: List[SceneGraph]\n",
    "    temporal_window: TemporalWindow\n",
    "    ego_poses: Dict[int, List[float]]\n",
    "\n",
    "def compute_spatial_relation(dx, dy, dz, dist):\n",
    "    h = \"right\" if dx > 0 else \"left\" if abs(dx) > abs(dy) else \"front\" if dy > 0 else \"back\"\n",
    "    v = \"above\" if dz > 0.3 else \"below\" if dz < -0.3 else \"level\"\n",
    "    d = \"near\" if dist < 3 else \"medium\" if dist < 8 else \"far\"\n",
    "    return f\"{h}_{v}_{d}\"\n",
    "\n",
    "def build_scene_graph(steps, frame_idx=0):\n",
    "    nodes = {}\n",
    "    for nid, step in steps.items():\n",
    "        pos = np.array([step.centroid.x, step.centroid.y, step.centroid.z])\n",
    "        nodes[nid] = SceneNode(node_id=nid, step=step, position=pos)\n",
    "    \n",
    "    edges = []\n",
    "    ids = sorted(nodes.keys())\n",
    "    for i, src_id in enumerate(ids):\n",
    "        for dst_id in ids[i+1:]:\n",
    "            delta = nodes[dst_id].position - nodes[src_id].position\n",
    "            dist = float(np.linalg.norm(delta))\n",
    "            if dist < 50:\n",
    "                rel = compute_spatial_relation(delta[0], delta[1], delta[2], dist)\n",
    "                edges.append(SceneEdge(src=src_id, dst=dst_id, distance=dist, relation=rel))\n",
    "    \n",
    "    return SceneGraph(nodes=nodes, edges=edges, frame_idx=frame_idx)\n",
    "\n",
    "print(\"Scene graph components defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 4DSG Serialization ====\n",
    "\n",
    "def serialize_4dsg(four_dsg):\n",
    "    \"\"\"Serialize 4DSG to text for VLM.\"\"\"\n",
    "    lines = [\"=== 4D Scene Graph ===\", \"\"]\n",
    "    \n",
    "    # Ego poses\n",
    "    lines.append(\"Ego Agent Trajectory:\")\n",
    "    for t, pose in sorted(four_dsg.ego_poses.items()):\n",
    "        lines.append(f\"  Frame {t}: position=({pose[0]:.2f}, {pose[1]:.2f}, {pose[2]:.2f})\")\n",
    "    lines.append(\"\")\n",
    "    \n",
    "    # Object tracks\n",
    "    lines.append(f\"Objects ({len(four_dsg.temporal_window.tracks)} tracked):\")\n",
    "    for tid, track in four_dsg.temporal_window.tracks.items():\n",
    "        lines.append(f\"\\n  Object {tid}:\")\n",
    "        for step in track.steps[-5:]:  # Last 5 observations\n",
    "            c = step.centroid\n",
    "            s = step.shape\n",
    "            size_x = s.x_max - s.x_min\n",
    "            size_y = s.y_max - s.y_min\n",
    "            size_z = s.z_max - s.z_min\n",
    "            lines.append(f\"    Position: ({c.x:.2f}, {c.y:.2f}, {c.z:.2f})\")\n",
    "            lines.append(f\"    Size: {size_x:.2f}m x {size_y:.2f}m x {size_z:.2f}m\")\n",
    "            lines.append(f\"    Visible: frames {step.temporal.t_start}-{step.temporal.t_end}\")\n",
    "    lines.append(\"\")\n",
    "    \n",
    "    # Spatial relations (from last frame)\n",
    "    if four_dsg.spatial_graphs:\n",
    "        last_sg = four_dsg.spatial_graphs[-1]\n",
    "        lines.append(f\"Spatial Relations (Frame {last_sg.frame_idx}):\")\n",
    "        for edge in last_sg.edges[:20]:\n",
    "            lines.append(f\"  Object {edge.src} is {edge.relation} of Object {edge.dst} ({edge.distance:.1f}m)\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "print(\"Serialization defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== VLM Client ====\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client(api_key=os.environ['GOOGLE_AI_API_KEY'])\n",
    "\n",
    "# Test connection\n",
    "response = client.models.generate_content(\n",
    "    model='gemma-3-4b-it',\n",
    "    contents='What is 2+2? Answer with just the number.',\n",
    ")\n",
    "print(f\"Gemma test response: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Point Cloud Generation (Simulated)\n",
    "\n",
    "Since MapAnything requires specific setup, we'll simulate point clouds from video frames using depth estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(url, cache_dir='video_cache'):\n",
    "    \"\"\"Download video from URL.\"\"\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    filename = url.split('/')[-1]\n",
    "    local_path = os.path.join(cache_dir, filename)\n",
    "    if not os.path.exists(local_path):\n",
    "        urllib.request.urlretrieve(url, local_path)\n",
    "    return local_path\n",
    "\n",
    "def extract_frames(video_path, num_frames=5):\n",
    "    \"\"\"Extract frames from video.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    indices = np.linspace(0, total - 1, min(num_frames, total), dtype=int).tolist()\n",
    "    \n",
    "    frames = []\n",
    "    for idx in indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def simulate_point_cloud(frame, num_points=5000):\n",
    "    \"\"\"Generate simulated point cloud from frame.\n",
    "    \n",
    "    This is a placeholder - in full SNOW pipeline, MapAnything would generate real 3D.\n",
    "    We simulate by creating random 3D points with color-based clustering.\n",
    "    \"\"\"\n",
    "    H, W = frame.shape[:2]\n",
    "    \n",
    "    # Sample random pixel locations\n",
    "    ys = np.random.randint(0, H, num_points)\n",
    "    xs = np.random.randint(0, W, num_points)\n",
    "    \n",
    "    # Create pseudo-depth based on image position (objects at bottom are closer)\n",
    "    depths = 5.0 + (ys / H) * 20.0  # 5-25m range\n",
    "    \n",
    "    # Convert to 3D (simple pinhole camera model)\n",
    "    fx, fy = W, H  # Simplified focal lengths\n",
    "    cx, cy = W/2, H/2\n",
    "    \n",
    "    X = (xs - cx) * depths / fx\n",
    "    Y = (ys - cy) * depths / fy\n",
    "    Z = depths\n",
    "    \n",
    "    points = np.stack([X, Y, Z], axis=1).astype(np.float32)\n",
    "    return points\n",
    "\n",
    "print(\"Video processing functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_snow_pipeline(video_path, question, num_frames=5):\n",
    "    \"\"\"Run full SNOW pipeline.\"\"\"\n",
    "    \n",
    "    # Configs\n",
    "    hdbscan_config = HDBSCANConfig(min_cluster_size=30)\n",
    "    hhop_config = HHopConfig()\n",
    "    tracker_config = TrackerConfig()\n",
    "    \n",
    "    # Step 1: Extract frames\n",
    "    frames = extract_frames(video_path, num_frames)\n",
    "    print(f\"  Extracted {len(frames)} frames\")\n",
    "    \n",
    "    # Step 2: Generate point clouds (simulated)\n",
    "    tracker = ObjectTracker(tracker_config)\n",
    "    spatial_graphs = []\n",
    "    \n",
    "    for frame_idx, frame in enumerate(frames):\n",
    "        # Get point cloud\n",
    "        points = simulate_point_cloud(frame)\n",
    "        print(f\"  Frame {frame_idx}: {len(points)} points\")\n",
    "        \n",
    "        # Cluster points\n",
    "        cluster_result = cluster_points(points, hdbscan_config)\n",
    "        print(f\"    Found {len(cluster_result.clusters)} clusters\")\n",
    "        \n",
    "        # Create STEP tokens\n",
    "        frame_steps = {}\n",
    "        for cluster_idx, cluster_indices in enumerate(cluster_result.clusters):\n",
    "            if len(cluster_indices) < 20:\n",
    "                continue\n",
    "            cluster_pts = points[cluster_indices]\n",
    "            mask = np.ones((32, 32), dtype=bool)\n",
    "            step = build_step_token(mask, cluster_pts, frame_idx, frame_idx + 1)\n",
    "            frame_steps[cluster_idx] = step\n",
    "        \n",
    "        # Filter implausible\n",
    "        filtered_steps = filter_implausible(frame_steps, hhop_config)\n",
    "        print(f\"    {len(filtered_steps)} valid objects after H-hop filter\")\n",
    "        \n",
    "        # Update tracker\n",
    "        tracker.update(filtered_steps, frame_idx)\n",
    "        \n",
    "        # Build spatial graph\n",
    "        sg = build_scene_graph(filtered_steps, frame_idx=frame_idx)\n",
    "        spatial_graphs.append(sg)\n",
    "    \n",
    "    # Get tracks and build 4DSG\n",
    "    tracks = tracker.get_tracks()\n",
    "    temporal_tracks = {\n",
    "        tid: TemporalTrack(track_id=tid, steps=t.steps)\n",
    "        for tid, t in tracks.items()\n",
    "    }\n",
    "    temporal_window = TemporalWindow(tracks=temporal_tracks)\n",
    "    ego_poses = {i: [0.0, i * 0.5, 0.0, 1.0, 0.0, 0.0, 0.0] for i in range(len(frames))}\n",
    "    \n",
    "    four_dsg = FourDSceneGraph(\n",
    "        temporal_window=temporal_window,\n",
    "        spatial_graphs=spatial_graphs,\n",
    "        ego_poses=ego_poses,\n",
    "    )\n",
    "    print(f\"  4DSG: {len(tracks)} tracks, {len(spatial_graphs)} frames\")\n",
    "    \n",
    "    # Serialize 4DSG\n",
    "    scene_text = serialize_4dsg(four_dsg)\n",
    "    \n",
    "    # Query Gemma\n",
    "    full_prompt = f\"\"\"You are a spatial reasoning assistant analyzing a 4D scene.\n",
    "\n",
    "{scene_text}\n",
    "\n",
    "Based on the scene information above, answer the following question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Think step by step about the spatial relationships and provide your final answer.\"\"\"\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model='gemma-3-4b-it',\n",
    "        contents=full_prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            max_output_tokens=1024,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return response.text, scene_text\n",
    "\n",
    "print(\"SNOW pipeline defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download VLM4D Data & Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download VLM4D benchmark data\n",
    "!mkdir -p data/vlm4d\n",
    "!wget -q https://huggingface.co/datasets/shijiezhou/VLM4D/resolve/main/real_mc.json -O data/vlm4d/real_mc.json\n",
    "\n",
    "with open('data/vlm4d/real_mc.json') as f:\n",
    "    queries = json.load(f)\n",
    "print(f\"Loaded {len(queries)} questions from real_mc.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation (limit samples for testing)\n",
    "MAX_SAMPLES = 10\n",
    "NUM_FRAMES = 5\n",
    "\n",
    "test_queries = queries[:MAX_SAMPLES]\n",
    "results = []\n",
    "\n",
    "for i, query in enumerate(tqdm(test_queries, desc=\"Processing\")):\n",
    "    print(f\"\\n[{i+1}/{len(test_queries)}] {query['id']}\")\n",
    "    \n",
    "    try:\n",
    "        video_path = download_video(query['video'])\n",
    "        \n",
    "        # Build question with choices\n",
    "        question = f\"{query['question']}\\n\"\n",
    "        for key, value in query['choices'].items():\n",
    "            question += f\"{key}. {value}\\n\"\n",
    "        question += \"\\nAnswer with the letter of your choice (A, B, C, or D).\"\n",
    "        \n",
    "        response, scene_text = run_snow_pipeline(video_path, question, NUM_FRAMES)\n",
    "        query['response'] = response\n",
    "        print(f\"  Response: {response[:100]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        query['response'] = f\"Error: {e}\"\n",
    "    \n",
    "    results.append(query)\n",
    "\n",
    "# Save results\n",
    "with open('snow_vlm4d_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"\\nSaved results to snow_vlm4d_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_answer(response, choices):\n",
    "    \"\"\"Extract answer from model response.\"\"\"\n",
    "    response_upper = response.upper()\n",
    "    \n",
    "    patterns = [\n",
    "        r'final answer[:\\s]+([A-D])',\n",
    "        r'answer[:\\s]+([A-D])',\n",
    "        r'\\(([A-D])\\)',\n",
    "        r'^([A-D])[.:\\s]',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, response, re.IGNORECASE | re.MULTILINE)\n",
    "        if match:\n",
    "            return match.group(1).upper()\n",
    "    \n",
    "    for char in response_upper:\n",
    "        if char in ['A', 'B', 'C', 'D']:\n",
    "            return char\n",
    "    \n",
    "    return ''\n",
    "\n",
    "def evaluate_results(results):\n",
    "    \"\"\"Compute accuracy metrics.\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for r in results:\n",
    "        if 'Error' in r.get('response', ''):\n",
    "            continue\n",
    "        \n",
    "        pred = extract_answer(r['response'], r['choices'])\n",
    "        gt = r['answer']\n",
    "        \n",
    "        if isinstance(gt, str) and len(gt) == 1:\n",
    "            gt_letter = gt.upper()\n",
    "        else:\n",
    "            gt_letter = None\n",
    "            for key, value in r['choices'].items():\n",
    "                if str(value).lower() == str(gt).lower():\n",
    "                    gt_letter = key.upper()\n",
    "                    break\n",
    "            if gt_letter is None:\n",
    "                gt_letter = str(gt).upper()\n",
    "        \n",
    "        is_correct = pred == gt_letter\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        print(f\"{r['id']}: pred={pred}, gt={gt_letter}, correct={is_correct}\")\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Accuracy: {accuracy:.2%} ({correct}/{total})\")\n",
    "    print(f\"{'='*50}\")\n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('snow_vlm4d_results.json')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
